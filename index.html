<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="author" content="Sicheng Xu">
    <meta name="description" content="Sicheng Xu's Homepage">
    <meta name="keywords" content="Sicheng Xu,徐思成,homepage,主页,computer vision,MSRA,3D generation,3D reconstruction">
    <title>Sicheng Xu's Homepage</title>
    <link rel="stylesheet" href="stylesheet.css">
</head>
<body>

<div class="container">
    
    <!-- Profile Section -->
    <section class="profile">
        <div class="profile-text">
            <h1 class="profile-name">Sicheng Xu</h1>
            <div class="profile-links">
                <a href="mailto:sichengxu@microsoft.com">Email</a> /
                <a href="https://scholar.google.com/citations?user=RKudwboAAAAJ&hl=en">Google Scholar</a> /
                <a href="https://github.com/sicxu">Github</a>
            </div>
            <p>
                I am currently a Senior Researcher at <strong>Microsoft Research Asia (MSRA)</strong>, which I joined in late 2021. 
                My research spans <strong>multimodal AIGC</strong>, <strong>3D vision</strong> and <strong>embodied AI</strong>, with an emphasis on digital humans, 3D generation and reconstruction. 
                <!-- Prior to joining MSRA, I spent a short period at Bytedance, where I developed real-time 3D face reconstruction and animation algorithms. -->
            </p>
            <p>
                <em style="color: red;">
                Feel free to email me if you’re interested in my research, collaboration, or internship opportunities.
                </em>
            </p>
        </div>
        <div class="profile-photo">
            <a href="assets/selfie.png">
                <img src="assets/selfie.png" alt="Sicheng Xu" class="profile-pic">
            </a>
        </div>
    </section>

    <!-- Publications Section -->
    <section class="publications">
        <h2>Publications</h2>
        <div class="pub-list">

            <div class="pub-item">
              <div class="pub-visual">
                  <img src="assets/figures/25_arxiv_trellis2.jpg" alt="Trellis.2">
              </div>
                <div class="pub-content">
                    <span class="paper-title">Gaussian Variation Field Diffusion for High-fidelity Video-to-4D Synthesis </span>
                    <div class="pub-authors">
                        Jianfeng Xiang<sup>+</sup>, Xiaoxue Chen<sup>+</sup>, <strong>Sicheng Xu</strong>, Ruicheng Wang<sup>+</sup>, Zelong Lv<sup>+</sup>, Yu Deng, Hongyuan Zhu, Yue Dong, Hao Zhao, Nicholas Jing Yuan, Jiaolong Yang
                    </div>
                    <div class="pub-info">
                        <a href="https://arxiv.org/pdf/2512.14692">[PDF]</a>
                        <a href="https://microsoft.github.io/TRELLIS.2/">[Project]</a>
                        <a href="https://github.com/microsoft/TRELLIS.2">[Code]</a>
                    </div>
                    <div class="pub-notes">
                        <sup>+</sup>: Intern at MSRA.
                    </div>
                </div>
            </div>

            <div class="pub-item">
              <div class="pub-visual">
                  <img src="assets/figures/26_iclr_mvrobobench.jpg" alt="mvrobobench">
              </div>
                <div class="pub-content">
                    <span class="paper-title">Seeing Across Views: Benchmarking Spatial Reasoning of Vision-Language Models in Robotic Scenes </span>
                    <div class="pub-authors">
                       Z. Feng, Z. Kang, Q. Wang, Z. Du, J. Yan, S. Shi, C. Yuan, H. Liang, Y. Deng, Q. Li, R. Yang, R. An, L. Zheng, W. Wang, S. Chen, <strong>S. Xu</strong>, Y. Liang, J. Yang, B. Guo
                    </div>
                    <div class="pub-info">
                        <a href="https://arxiv.org/pdf/2510.19400">[PDF]</a>
                        <a href="https://github.com/microsoft/MV-RoboBench">[Benchmark & Eval Code]</a>
                        <em>ICLR 2026</em>
                    </div>
                </div>
            </div>

            <div class="pub-item">
              <div class="pub-visual">
                  <img src="assets/figures/26_icra_vitra.jpg" alt="VITRA">
              </div>
                <div class="pub-content">
                    <span class="paper-title">Scalable Vision-Language-Action Model Pretraining for Robotic Manipulation with Real-Life Human Activity Videos </span>
                    <div class="pub-authors">
                       Q. Li, Y. Deng, Y. Liang, L. Luo, L. Zhou, C. Yao, L. Zeng, Z. Feng, H. Liang, <strong>S. Xu</strong>, Y. Zhang, X. Chen, H. Chen, L. Sun, D. Chen, J. Yang, B. Guo
                    </div>
                    <div class="pub-info">
                        <a href="https://arxiv.org/pdf/2510.21571">[PDF]</a>
                        <a href="https://microsoft.github.io/VITRA/">[Project]</a>
                        <a href="https://github.com/microsoft/VITRA/">[Code]</a>
                        <em>ICRA 2026</em>
                    </div>
                </div>
            </div>

            <div class="pub-item">
              <div class="pub-visual">
                  <img src="assets/figures/25_neurips_vasa3d.jpg" alt="VASA-3D">
              </div>
                <div class="pub-content">
                    <span class="paper-title">VASA-3D: Lifelike Audio-Driven Gaussian Head Avatars from a Single Image </span>
                    <div class="pub-authors">
                        <strong>Sicheng Xu*</strong>, Guojun Chen*, Jiaolong Yang, Yizhong Zhang, Stephen Lin, Baining Guo
                    </div>
                    <div class="pub-info">
                        <a href="https://arxiv.org/abs/2512.14677">[PDF]</a>
                        <a href="https://www.microsoft.com/en-us/research/project/vasa-3d/">[Project]</a>
                        <em>NeurIPS 2025</em>
                    </div>
                    <div class="pub-notes">
                        *: Equal contributions.
                    </div>
                </div>
            </div>

            <div class="pub-item">
              <div class="pub-visual">
                  <img src="assets/figures/25_neurips_moge2.jpg" alt="MoGe2">
              </div>
                <div class="pub-content">
                    <span class="paper-title">MoGe-2: Accurate Monocular Geometry with Metric Scale and Sharp Details </span>
                    <div class="pub-authors">
                       Ruicheng Wang<sup>+</sup>, <strong>Sicheng Xu</strong>, Yue Dong, Yu Deng, Jianfeng Xiang<sup>+</sup>, Zelong Lv<sup>+</sup>, Guangzhong Sun, Xin Tong, Jiaolong Yang
                    </div>
                    <div class="pub-info">
                        <a href="https://arxiv.org/pdf/2507.02546">[PDF]</a>
                        <a href="https://wangrc.site/MoGe2Page/">[Project]</a>
                        <a href="https://github.com/microsoft/moge">[Code]</a>
                        <em>NeurIPS 2025</em>
                    </div>
                    <div class="pub-notes">
                        <sup>+</sup>: Intern at MSRA.
                    </div>
                </div>
            </div>

            <div class="pub-item">
              <div class="pub-visual">
                  <img src="assets/figures/25_iccv_gvfdiffusion.jpg" alt="GVFDiffusion">
              </div>
                <div class="pub-content">
                    <span class="paper-title">Gaussian Variation Field Diffusion for High-fidelity Video-to-4D Synthesis </span>
                    <div class="pub-authors">
                        Bowen Zhang<sup>+</sup>, <strong>Sicheng Xu</strong>, Chuxin Wang, Jiaolong Yang, Feng Zhao, Dong Chen, Baining Guo
                    </div>
                    <div class="pub-info">
                        <a href="https://arxiv.org/abs/2507.23785">[PDF]</a>
                        <a href="https://gvfdiffusion.github.io/">[Project]</a>
                        <a href="https://github.com/ForeverFancy/gvfdiffusion">[Code]</a>
                        <em>ICCV 2025</em>
                    </div>
                    <div class="pub-notes">
                        <sup>+</sup>: Intern at MSRA.
                    </div>
                </div>
            </div>

            <div class="pub-item">
              <div class="pub-visual">
                  <img src="assets/figures/25_vr_vasarig.jpg" alt="VASA-Rig">
              </div>
                <div class="pub-content">
                    <span class="paper-title">VASA-Rig: Audio-Driven 3D Facial Animation with 'Live' Mood Dynamics in Virtual Reality </span>
                    <div class="pub-authors">
                        Ye Pan, Chang Liu, <strong>Sicheng Xu</strong>, Shuai Tan, Jiaolong Yang
                    </div>
                    <div class="pub-info">
                        <a href="https://ieeexplore.ieee.org/document/10916977">[PDF]</a>
                        <em>VR 2025&TVCG, <span class="tag-award">Best Paper Honorable Mention Award</span></em>
                    </div>
                    <div class="pub-notes">
                    </div>
                </div>
            </div>


            <div class="pub-item">
                <!-- <div class="pub-visual">
                    <video playsinline autoplay loop preload muted>
                        <source src="assets/trellis.mp4" type="video/mp4">
                    </video>
                </div> -->
                <div class="pub-visual">
                    <img src="assets/figures/25_cvpr_trellis.jpg" alt="Trellis">
                </div>
                <div class="pub-content">
                    <span class="paper-title">Structured 3D Latents for Scalable and Versatile 3D Generation </span>
                    <div class="pub-authors">
                        Jianfeng Xiang<sup>+</sup>, Zelong Lv<sup>+</sup>, <strong>Sicheng Xu</strong>, Yu Deng, Ruicheng Wang<sup>+</sup>, Bowen Zhang<sup>+</sup>, Dong Chen, Xin Tong, Jiaolong Yang
                    </div>
                    <div class="pub-info">
                        <a href="https://arxiv.org/abs/2412.01506">[PDF]</a>
                        <a href="https://trellis3d.github.io/">[Project]</a>
                        <a href="https://github.com/Microsoft/TRELLIS">[Code]</a>
                        <em>CVPR 2025, <span class="tag-spotlight">Spotlight</span></em>
                    </div>
                    <div class="pub-notes">
                        <sup>+</sup>: Intern at MSRA.
                    </div>
                </div>
            </div>

            <div class="pub-item">
                <div class="pub-visual">
                    <img src="assets/figures/25_cvpr_moge.jpg" alt="CogACT">
                </div>
                <div class="pub-content">
                    <span class="paper-title">MoGe: Unlocking Accurate Monocular Geometry Estimation for Open-Domain Images with Optimal Training Supervision </span>
                    <div class="pub-authors">
                        Ruicheng Wang<sup>+</sup>, <strong>Sicheng Xu</strong>, Cassie Dai<sup>+</sup>, Jianfeng Xiang<sup>+</sup>, Yu Deng, Xin Tong, Jiaolong Yang
                    </div>
                    <div class="pub-info">
                        <a href="https://arxiv.org/abs/2410.19115">[PDF]</a>
                        <a href="https://wangrc.site/MoGePage/">[Project]</a>
                        <a href="https://github.com/microsoft/moge">[Code]</a>
                        <em>CVPR 2025, <span class="tag-oral">Oral Presentation</span></em>
                    </div>
                    <div class="pub-notes">
                        <sup>+</sup>: Intern at MSRA.
                    </div>
                </div>
            </div>

            <div class="pub-item">
                <div class="pub-visual">
                    <img src="assets/figures/24_arxiv_CogACT.jpg" alt="CogACT">
                </div>
                <div class="pub-content">
                    <span class="paper-title">CogACT: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation </span>
                    <div class="pub-authors">
                        Q. Li, Y. Liang, Z. Wang, L. Luo, X. Chen, M. Liao, F. Wei, Yu Deng, <strong>S. Xu</strong>, Y. Zhang, X. Wang, B. Liu, J. Fu, J. Bao, D. Chen, Y. Shi, J. Yang, B. Guo
                    </div>
                    <div class="pub-info">
                        <a href="https://arxiv.org/pdf/2411.19650.pdf">[PDF]</a>
                        <a href="https://cogact.github.io/">[Project]</a>
                        <a href="https://github.com/microsoft/CogACT">[Code]</a>
                        <!-- <em>Arxiv</em> -->
                    </div>
                    <div class="pub-notes">
                        *: Equal contributions.
                    </div>
                </div>
            </div>

            <div class="pub-item">
                <div class="pub-visual">
                    <img src="assets/figures/24_neurips_vasa1.jpg" alt="Vasa-1">
                </div>
                <div class="pub-content">
                    <span class="paper-title">Vasa-1: Lifelike audio-driven talking faces generated in real time </span>
                    <div class="pub-authors">
                        <strong>Sicheng Xu*</strong>, Guojun Chen*, Yu-Xiao Guo*, Jiaolong Yang*, Chong Li, Zhenyu Zang, Yizhong Zhang, Xin Tong, Baining Guo
                    </div>
                    <div class="pub-info">
                        <a href="https://arxiv.org/abs/2404.10667">[PDF]</a>
                        <a href="https://www.microsoft.com/en-us/research/project/vasa-1/">[Project]</a>
                        <a href="mailto:sichengxu@microsoft.com" title="please email to me for model access.">[CAPP]</a>
                        <em>NeurIPS 2024, <span class="tag-oral">Oral Presentation</span></em>
                    </div>
                    <div class="pub-notes">
                        *: Equal contributions.
                    </div>
                </div>
            </div>

            <div class="pub-item">
                <div class="pub-visual">
                    <img src="assets/figures/23_sa_aniportraitgan.jpg" alt="AniPortraitGAN">
                </div>
                <div class="pub-content">
                    <span class="paper-title">AniPortraitGAN: Animatable 3D Portrait Generation from 2D Image Collections </span>
                    <div class="pub-authors">
                        Yue Wu<sup>+</sup>*, <strong>Sicheng Xu*</strong>, Jianfeng Xiang, Fangyun Wei, Qifeng Chen, Jiaolong Yang, Xin Tong.
                    </div>
                    <div class="pub-info">
                        <a href="https://arxiv.org/abs/2309.02186">[PDF]</a>
                        <a href="https://kathrinawu.github.io/AniPortraitGAN/">[Project]</a>
                        <a href="https://github.com/kathrinawu/AniPortraitGAN">[Code]</a>
                        <em>SIGGRAPH Asia 2023</em>
                    </div>
                    <div class="pub-notes">
                        <sup>+</sup>: Intern at MSRA, *: Equal contributions.
                    </div>
                </div>
            </div>

            <div class="pub-item">
                <div class="pub-visual">
                    <img src="assets/figures/20_cvpr_deep3dportrait.png" alt="Deep 3D Portrait">
                </div>
                <div class="pub-content">
                    <span class="paper-title">Deep 3D Portrait from a Single Image</span>
                    <div class="pub-authors">
                        <strong>Sicheng Xu</strong>, Jiaolong Yang, Dong Chen, Fang Wen, Yu Deng, Yunde Jia, Xin Tong
                    </div>
                    <div class="pub-info">
                        <a href="https://arxiv.org/abs/2004.11598">[PDF]</a>
                        <a href="https://github.com/sicxu/Deep3dPortrait">[Code]</a>
                        <em>CVPR 2020</em>
                    </div>
                    <div class="pub-notes">
                        Work done during my internship at MSRA.
                    </div>
                </div>
            </div>

            <div class="pub-item">
                <div class="pub-visual">
                    <img src="assets/figures/19_cvprw_3dface.png" alt="Accurate 3D Face Reconstruction">
                </div>
                <div class="pub-content">
                    <span class="paper-title">Accurate 3D Face Reconstruction with Weakly-Supervised Learning: From Single Image to Image Set</span>
                    <div class="pub-authors">
                        Yu Deng, Jiaolong Yang, <strong>Sicheng Xu</strong>, Dong Chen, Yunde Jia, Xin Tong
                    </div>
                    <div class="pub-info">
                        <a href="https://arxiv.org/abs/1903.08527">[PDF]</a>
                        <a href="https://github.com/sicxu/Deep3DFaceRecon_pytorch">[Code]</a>
                        <em>CVPRW 2019, <span class="tag-award">Best Paper Award</span></em>
                    </div>
                    <div class="pub-notes">
                        Work done during my internship at MSRA.
                    </div>
                </div>
            </div>

        </div>
    </section>

    <!-- Services Section -->
    <section class="services">
        <h2>Academic Services</h2>
        <p>
            <strong>Conference Reviewer:</strong> CVPR, ICCV, ECCV, NeurIPS, SIGGRAPH, SIGGRAPH Asia
            <br>
            <strong>Journal Reviewer:</strong> TPAMI, TVCG, IJCV, TIP
        </p>
    </section>

    <!-- Footer -->
    <footer>
        <p>The website was vibe-coded together with Gemini 3 Pro ✨.</p>
    </footer>

</div>

</body>
</html>
